# Wedge Project

# This project is a view into the world of data engineering. As a graduate of the MSBA, you are going to spend a ton of your time trying to shape up data sets for analyses. It’s literally 80% of the job. A deep understanding of data pipelines and how raw data is transformed into consumable data is critical to a successful career. As you work with data you will often be called upon to define the data sets you need for your work. This project hones those skills.

# The Wedge Co-Op is the largest co-operative grocery store in the US and is in Minneapolis, MN. Through a partnership with the co-op, we have data dating back to January 1, 2010 from the point-of-sale (POS) system that the Wedge developed . We have data through January 2017. This system logs every row of every receipt. While we do not have other sources of data such as ordering, inventory, and spoilage, this transaction-level view is one of the richest data sets available to us. The source of this richness is that the Wedge is a member-owned co-operative. The public is welcome to shop there, but membership is lightweight: $80 (which can be paid in installments) buys you shares which entitle you to $3.75 off per month, other Member discounts, and annual refunds based on business profitability. 

# About 75% of transactions are generated by owners. For those owners , we have a full record of their shopping at the co-op and can understand the changes to their consumption over time and can add dimensions like products and departments shopped. 

# On the other hand, having raw POS records comes with its own challenges. For instance, the transaction records contain many records that are not item purchases like payment, tax, discounts, members rounding up for charity and change. So, the data is valuable, but we’re going to have to do some work.


# Task 1: Building a Transaction Database in Google Big Query

# Google Big Query is a distributed data warehouse built on a serverless architecture . In this task you’ll upload all Wedge transaction records to Google Big Query. You’ll want to make sure that the column data types are correctly specified and you’ve properly handled the null values. 


# Task 2: A Sample of Owners
# Deliverable
# A python script that handles the following tasks: 
1.	Connects to your GBQ instance.
2.	Builds a list of owners. 
3.	Takes a sample of the owners. 
4.	Extracts all records associated with those owners and writes them to a local text file. 

# Task 3: Building Summary Tables

It is useful to have summary files that allow you to quickly answer questions such as the following:
•	How have our sales-by-day changed over the last few months?
•	What is our most popular item in each department?
•	Which owners spend the most per month in each department?

For this task, you will build a single SQLite database via Python (in a .db file) containing three tables:
1.	Sales by date by hour: By calendar date (YYYY-MM-DD) and hour of the day, determine the total spend in the store, the number of transactions, and a count of the number of items . 
2.	Sales by owner by year by month: A file that has the following columns: card_no, year, month, sales, transactions, and items. 
3.	Sales by product description by year by month: A file that has the following columns: upc, description, department number, department name, year, month, sales, transactions, and items.
You will submit your Python code that builds the database. 

